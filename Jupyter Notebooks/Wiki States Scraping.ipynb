{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li class=\"list-group-item col-md-3\" id=\"race1\" style=\"min-height:500px\"><div><p class=\"alert alert-info\" style=\"text-align:center\">Races in San Diego, CA (2016)</p><div><img alt=\"San Diego races chart\" src=\"http://pics.city-data.com/craces2/2448.jpg\" style=\"width:100%;max-width:350px\"/>\n",
      "</div></div>\n",
      "</li>, <li class=\"list-group-item col-md-3\" style=\"min-height:500px\"><ul class=\"list-group\"><li class=\"list-group-item\"><span class=\"badge\">602,167</span><span class=\"badge alert-info\">42.8%</span><b>White alone</b></li>\n",
      "<li class=\"list-group-item\"><span class=\"badge\">421,965</span><span class=\"badge alert-info\">30.0%</span><b>Hispanic</b></li>\n",
      "<li class=\"list-group-item\"><span class=\"badge\">236,310</span><span class=\"badge alert-info\">16.8%</span><b>Asian alone</b></li>\n",
      "<li class=\"list-group-item\"><span class=\"badge\">86,405</span><span class=\"badge alert-info\">6.1%</span><b>Black alone</b></li>\n",
      "<li class=\"list-group-item\"><span class=\"badge\">50,895</span><span class=\"badge alert-info\">3.6%</span><b>Two or more races</b></li>\n",
      "<li class=\"list-group-item\"><span class=\"badge\">4,907</span><span class=\"badge alert-info\">0.3%</span><b>Native Hawaiian and Other<br/>Pacific Islander alone</b></li>\n",
      "<li class=\"list-group-item\"><span class=\"badge\">1,958</span><span class=\"badge alert-info\">0.1%</span><b>American Indian alone</b></li>\n",
      "<li class=\"list-group-item\"><span class=\"badge\">2,015</span><span class=\"badge alert-info\">0.1%</span><b>Other race alone</b></li>\n",
      "</ul></li>, <li class=\"list-group-item\"><span class=\"badge\">602,167</span><span class=\"badge alert-info\">42.8%</span><b>White alone</b></li>, <li class=\"list-group-item\"><span class=\"badge\">421,965</span><span class=\"badge alert-info\">30.0%</span><b>Hispanic</b></li>, <li class=\"list-group-item\"><span class=\"badge\">236,310</span><span class=\"badge alert-info\">16.8%</span><b>Asian alone</b></li>, <li class=\"list-group-item\"><span class=\"badge\">86,405</span><span class=\"badge alert-info\">6.1%</span><b>Black alone</b></li>, <li class=\"list-group-item\"><span class=\"badge\">50,895</span><span class=\"badge alert-info\">3.6%</span><b>Two or more races</b></li>, <li class=\"list-group-item\"><span class=\"badge\">4,907</span><span class=\"badge alert-info\">0.3%</span><b>Native Hawaiian and Other<br/>Pacific Islander alone</b></li>, <li class=\"list-group-item\"><span class=\"badge\">1,958</span><span class=\"badge alert-info\">0.1%</span><b>American Indian alone</b></li>, <li class=\"list-group-item\"><span class=\"badge\">2,015</span><span class=\"badge alert-info\">0.1%</span><b>Other race alone</b></li>, <li class=\"list-group-item list-group-item-info\">National Bridge Inventory (NBI) Statistics</li>, <li class=\"list-group-item\"><span class=\"badge\">1,303</span>Number of bridges</li>, <li class=\"list-group-item\"><span class=\"badge\">20,946m</span><span class=\"badge\">68,720ft</span>Total length</li>, <li class=\"list-group-item\"><span class=\"badge\"> $231,239,000</span>Total costs</li>, <li class=\"list-group-item\"><span class=\"badge\">68,326,521</span>Total average daily traffic</li>, <li class=\"list-group-item\"><span class=\"badge\">2,571,281</span>Total average daily truck traffic</li>, <li class=\"list-group-item\"><span class=\"badge\">3</span><b>1900-1909</b></li>, <li class=\"list-group-item\"><span class=\"badge\">6</span><b>1910-1919</b></li>, <li class=\"list-group-item\"><span class=\"badge\">6</span><b>1920-1929</b></li>, <li class=\"list-group-item\"><span class=\"badge\">13</span><b>1930-1939</b></li>, <li class=\"list-group-item\"><span class=\"badge\">37</span><b>1940-1949</b></li>, <li class=\"list-group-item\"><span class=\"badge\">103</span><b>1950-1959</b></li>, <li class=\"list-group-item\"><span class=\"badge\">383</span><b>1960-1969</b></li>, <li class=\"list-group-item\"><span class=\"badge\">380</span><b>1970-1979</b></li>, <li class=\"list-group-item\"><span class=\"badge\">150</span><b>1980-1989</b></li>, <li class=\"list-group-item\"><span class=\"badge\">104</span><b>1990-1999</b></li>, <li class=\"list-group-item\"><span class=\"badge\">89</span><b>2000-2009</b></li>, <li class=\"list-group-item\"><span class=\"badge\">29</span><b>2010-2015</b></li>]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'findAll'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ec4fbf8c4c00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m#print(rows)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1883\u001b[0m         raise AttributeError(\n\u001b[1;32m-> 1884\u001b[1;33m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1885\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'findAll'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests  \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Providing the URL from which web scraping will be done\n",
    "url = 'http://www.city-data.com/city/San-Diego-California.html'\n",
    "response =  requests.get(url)\n",
    "\n",
    "#For parsing\n",
    "page_soup = soup(response.text, \"html.parser\")\n",
    "\n",
    "#for record in page_soup.findAll('tr'):\n",
    "  #  print (record.text)\n",
    "\n",
    "table = page_soup.findAll(\"li\",{\"class\":\"list-group-item\"})\n",
    "#table = containers.text\n",
    "print (table)\n",
    "\n",
    "rows = table.findAll(\"tr\")\n",
    "#print(rows)\n",
    "    \n",
    "columns = [v.text.replace('\\n',\"\") for v in rows[0].find_all(\"th\")]\n",
    "#print (columns)\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(1, len(rows)):\n",
    "    tds = rows[i].find_all(\"td\")\n",
    "    #print (tds)\n",
    "   \n",
    "       \n",
    "    values = [tds[0].text.replace(\"\\n\",\"\"),tds[1].text.replace(\"\\n\",\"\").replace(\"[d]\",\"\"), tds[2].text.replace(\"\\n\",\"\").replace(\"\\xa0\",\"\"), tds[3].text.replace(\"\\n\",\"\"), tds[4].text.replace(\"\\n\",\"\"), tds[5].text.replace(\"\\n\",\"\"), tds[6].text.replace(\"\\xa0sq\\xa0mi\\n\",\"\"), tds[7].text.replace(\"\\xa0km2\\n\",\"\"),tds[8].text.replace(\"\\xa0mi\\n\",\"\")]\n",
    "        \n",
    "    #print (values)\n",
    "  \n",
    "  \n",
    "    df = df.append(pd.Series(values, index=columns),ignore_index = True)\n",
    "    print (df)\n",
    "    \n",
    "\n",
    "    \n",
    "#------------------------------------\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "\n",
    "def webScrapping(line):\n",
    "    uClient = uReq(line)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "\n",
    "    #for parsing\n",
    "    page_soup = soup(page_html, \"html.parser\")\n",
    "\n",
    "   \n",
    "    #grabs each product\n",
    "\n",
    "\n",
    "    birth_date1 = page_soup.find(\"span\",{\"class\":\"bday\"})\n",
    "    # = str(birth_date)\n",
    "    birth_date = (birth_date1.text)\n",
    "    print (birth_date)\n",
    "\n",
    "    birth_place1 = page_soup.find(\"div\",{\"class\":\"birthplace\"})\n",
    "    birth_place = (birth_place1.text)\n",
    "    print (birth_place)\n",
    "\n",
    "    role1= page_soup.find(\"td\",{\"class\":\"role\"})\n",
    "    role = (role1.text).replace(\"\\n\",\" \")\n",
    "    print (role)\n",
    "\n",
    "    f.write((birth_date) + \",\" + (birth_place).replace(\",\",\" \") + \",\" + (role).replace(\",\",\" \") + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New-York\n",
      "Los-Angeles\n",
      "Chicago\n",
      "Houston\n",
      "Phoenix\n",
      "Philadelphia\n",
      "San-Antonio\n",
      "San-Diego\n",
      "Dallas\n",
      "San-Jose\n",
      "Austin\n",
      "Jacksonville\n",
      "San-Francisco\n",
      "Columbus\n",
      "Fort-Worth\n",
      "Indianapolis\n",
      "Charlotte\n",
      "Seattle\n",
      "Denver\n",
      "Washington-D.C.\n",
      "Boston\n",
      "El-Paso\n",
      "Detroit\n",
      "Nashville\n",
      "Memphis\n",
      "Portland\n",
      "Oklahoma\n",
      "Las-Vegas\n",
      "Louisville\n",
      "Baltimore\n",
      "Milwaukee\n",
      "Albuquerque\n",
      "Tucson\n",
      "Fresno\n",
      "Sacramento\n",
      "Mesa\n",
      "Kansas\n",
      "Atlanta\n",
      "Long-Beach\n",
      "Omaha\n",
      "Raleigh\n",
      "Colorado-Springs\n",
      "Miami\n",
      "Virginia-Beach\n",
      "Oakland\n",
      "Minneapolis\n",
      "Tulsa\n",
      "Arlington\n",
      "New-Orleans\n",
      "Wichita\n",
      "Cleveland\n",
      "Tampa\n",
      "Bakersfield\n",
      "Aurora\n",
      "Anaheim\n",
      "Honolulu\n",
      "Santa-Ana\n",
      "Riverside\n",
      "Corpus-Christi\n",
      "Lexington\n",
      "Stockton\n",
      "St.-Louis\n",
      "Saint-Paul\n",
      "Henderson\n",
      "Pittsburgh\n",
      "Cincinnati\n",
      "Anchorage\n",
      "Greensboro\n",
      "Plano\n",
      "Newark\n",
      "Lincoln\n",
      "Orlando\n",
      "Irvine\n",
      "Toledo\n",
      "Jersey\n",
      "Chula-Vista\n",
      "Durham\n",
      "Fort-Wayne\n",
      "St.-Petersburg\n",
      "Laredo\n",
      "Buffalo\n",
      "Madison\n",
      "Lubbock\n",
      "Chandler\n",
      "Scottsdale\n",
      "Reno\n",
      "Glendale\n",
      "Norfolk\n",
      "Winstonâ€“Salem\n",
      "North-Las-Vegas\n",
      "Gilbert\n",
      "Chesapeake\n",
      "Irving\n",
      "Hialeah\n",
      "Garland\n",
      "Fremont\n",
      "Richmond\n",
      "Boise\n",
      "Baton-Rouge\n",
      "Des-Moines\n",
      "Spokane\n",
      "San-Bernardino\n",
      "Modesto\n",
      "Tacoma\n",
      "Fontana\n",
      "Santa-Clarita\n",
      "Birmingham\n",
      "Oxnard\n",
      "Fayetteville\n",
      "Rochester\n",
      "Moreno-Valley\n",
      "Glendale\n",
      "Yonkers\n",
      "Huntington-Beach\n",
      "Aurora\n",
      "Salt-Lake\n",
      "Amarillo\n",
      "Montgomery\n",
      "Grand-Rapids\n",
      "Little-Rock\n",
      "Akron\n",
      "Augusta\n",
      "Huntsville\n",
      "Columbus\n",
      "Grand-Prairie\n",
      "Shreveport\n",
      "Overland-Park\n",
      "Tallahassee\n",
      "Mobile\n",
      "Port-St.-Lucie\n",
      "Knoxville\n",
      "Worcester\n",
      "Tempe\n",
      "Cape-Coral\n",
      "Brownsville\n",
      "McKinney\n",
      "Providence\n",
      "Fort-Lauderdale\n",
      "Newport-News\n",
      "Chattanooga\n",
      "Rancho-Cucamonga\n",
      "Frisco\n",
      "Sioux-Falls\n",
      "Oceanside\n",
      "Ontario\n",
      "Vancouver\n",
      "Santa-Rosa\n",
      "Garden-Grove\n",
      "Elk-Grove\n",
      "Pembroke-Pines\n",
      "Salem\n",
      "Eugene\n",
      "Peoria\n",
      "Corona\n",
      "Springfield\n",
      "Jackson\n",
      "Cary\n",
      "Fort-Collins\n",
      "Hayward\n",
      "Lancaster\n",
      "Alexandria\n",
      "Salinas\n",
      "Palmdale\n",
      "Lakewood\n",
      "Springfield\n",
      "Sunnyvale\n",
      "Hollywood\n",
      "Pasadena\n",
      "Clarksville\n",
      "Pomona\n",
      "Kansas\n",
      "Macon\n",
      "Escondido\n",
      "Paterson\n",
      "Joliet\n",
      "Naperville\n",
      "Rockford\n",
      "Torrance\n",
      "Bridgeport\n",
      "Savannah\n",
      "Killeen\n",
      "Bellevue\n",
      "Mesquite\n",
      "Syracuse\n",
      "McAllen\n",
      "Pasadena\n",
      "Orange\n",
      "Fullerton\n",
      "Dayton\n",
      "Miramar\n",
      "Olathe\n",
      "Thornton\n",
      "Waco\n",
      "Murfreesboro\n",
      "Denton\n",
      "West-Valley\n",
      "Midland\n",
      "Carrollton\n",
      "Roseville\n",
      "Warren\n",
      "Charleston\n",
      "Hampton\n",
      "Surprise\n",
      "Columbia\n",
      "Coral-Springs\n",
      "Visalia\n",
      "Sterling-Heights\n",
      "Gainesville\n",
      "Cedar-Rapids\n",
      "New-Haven\n",
      "Stamford\n",
      "Elizabeth\n",
      "Concord\n",
      "Thousand-Oaks\n",
      "Kent\n",
      "Santa-Clara\n",
      "Simi-Valley\n",
      "Lafayette\n",
      "Topeka\n",
      "Athens\n",
      "Round-Rock\n",
      "Hartford\n",
      "Norman\n",
      "Victorville\n",
      "Fargo\n",
      "Berkeley\n",
      "Vallejo\n",
      "Abilene\n",
      "Columbia\n",
      "Ann-Arbor\n",
      "Allentown\n",
      "Pearland\n",
      "Beaumont\n",
      "Wilmington\n",
      "Evansville\n",
      "Arvada\n",
      "Provo\n",
      "Independence\n",
      "Lansing\n",
      "Odessa\n",
      "Richardson\n",
      "Fairfield\n",
      "El-Monte\n",
      "Rochester\n",
      "Clearwater\n",
      "Carlsbad\n",
      "Springfield\n",
      "Temecula\n",
      "West-Jordan\n",
      "Costa-Mesa\n",
      "Miami-Gardens\n",
      "Cambridge\n",
      "College-Station\n",
      "Murrieta\n",
      "Downey\n",
      "Peoria\n",
      "Westminster\n",
      "Elgin\n",
      "Antioch\n",
      "Palm-Bay\n",
      "High-Point\n",
      "Lowell\n",
      "Manchester\n",
      "Pueblo\n",
      "Gresham\n",
      "North-Charleston\n",
      "Ventura\n",
      "Inglewood\n",
      "Pompano-Beach\n",
      "Centennial\n",
      "West-Palm-Beach\n",
      "Everett\n",
      "Richmond\n",
      "Clovis\n",
      "Billings\n",
      "Waterbury\n",
      "Broken-Arrow\n",
      "Lakeland\n",
      "West-Covina\n",
      "Boulder\n",
      "Daly\n",
      "Santa-Maria\n",
      "Hillsboro\n",
      "Sandy-Springs\n",
      "Norwalk\n",
      "Jurupa-Valley\n",
      "Lewisville\n",
      "Greeley\n",
      "Davie\n",
      "Green-Bay\n",
      "Tyler\n",
      "League\n",
      "Burbank\n",
      "San-Mateo\n",
      "Wichita-Falls\n",
      "El-Cajon\n",
      "Rialto\n",
      "Lakewood\n",
      "Edison\n",
      "Davenport\n",
      "South-Bend\n",
      "Woodbridge\n",
      "Las-Cruces\n",
      "Vista\n",
      "Renton\n",
      "Sparks\n",
      "Clinton\n",
      "Allen\n",
      "Tuscaloosa\n",
      "San-Angelo\n",
      "Vacaville\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Csv =pd.read_csv('\\\\NIKITA-PC\\Users\\Nikita\\\\statesWiki.csv') \n",
    "#Csv.head(400)\n",
    "for row in Csv.head(400).itertuples():\n",
    "    city = row.City.replace(\" \",\"-\").replace(\",\",\"\")\n",
    "    print(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Diego\n",
      "California\n",
      "602,167\n",
      "42.8%\n",
      "White alone\n",
      "421,965\n",
      "30.0%\n",
      "Hispanic\n",
      "236,310\n",
      "16.8%\n",
      "Asian alone\n",
      "86,405\n",
      "6.1%\n",
      "Black alone\n",
      "50,895\n",
      "3.6%\n",
      "Two or more races\n",
      "4,907\n",
      "0.3%\n",
      "Native Hawaiian and OtherPacific Islander alone\n",
      "1,958\n",
      "0.1%\n",
      "American Indian alone\n",
      "2,015\n",
      "0.1%\n",
      "Other race alone\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests  \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "filename = \"CityScrape.csv\"\n",
    "f = open(filename, \"w\")\n",
    "\n",
    "headers = \"State, City, NameOFRace, NumberOfpeople, PercentOfPeople\\n\"\n",
    "#For parsing\n",
    "f.write(headers)\n",
    "page_soup = soup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "###################3\n",
    "\n",
    "\n",
    "#Providing the URL from which web scraping will be done\n",
    "url = 'http://www.city-data.com/city/San-Diego-California.html'\n",
    "response =  requests.get(url)\n",
    "\n",
    "\n",
    "\n",
    "#for record in page_soup.findAll('tr'):\n",
    "  #  print (record.text)\n",
    "\n",
    "table = page_soup.findAll(\"li\",{\"class\":\"list-group-item col-md-3\"})\n",
    "#table = containers.text\n",
    "\n",
    "#print (table)\n",
    "Place = page_soup.find(\"h1\",{\"class\":\"city\"})\n",
    "placeText=Place.text\n",
    "City,State = placeText.split(', ', 1)\n",
    "print(City)\n",
    "print(State)\n",
    "\n",
    "for i in range (len(table)):\n",
    "    tds = table[i].findAll(\"li\",{\"class\":\"list-group-item\"})\n",
    "    #print (tds)\n",
    "    for j in range (len(tds)):\n",
    "        \n",
    "        NumberOfpeople = tds[j].find(\"span\",{\"class\":\"badge\"})\n",
    "        no = NumberOfpeople.text\n",
    "        print(no)\n",
    "    \n",
    "        PercentOfPeople = tds[j].find(\"span\",{\"class\":\"badge alert-info\"})\n",
    "        pp = PercentOfPeople.text\n",
    "        print  (PercentOfPeople.text)\n",
    "   \n",
    "        NameOFRace = tds[j].b\n",
    "        nr = NameOFRace.text\n",
    "        print (NameOFRace.text)\n",
    "    \n",
    "        f.write((State) + \",\" + (City) + \",\" + (nr) + \",\" + (no).replace(\",\",\"\") + \",\" + (pp) + \"\\n\")\n",
    "f.close()    \n",
    "    \n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
